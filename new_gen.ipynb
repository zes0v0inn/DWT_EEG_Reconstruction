{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:16:37.945013Z",
     "end_time": "2025-01-03T10:16:38.943741Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import clip\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "train = False\n",
    "classes = None\n",
    "pictures= None\n",
    "\n",
    "def load_data():\n",
    "    data_list = []\n",
    "    label_list = []\n",
    "    texts = []\n",
    "    images = []\n",
    "\n",
    "    if train:\n",
    "        text_directory = \"/media/hanakawalab/Transcend/THINGS-Data/THINGS-EEG_images_set/training_images\"\n",
    "    else:\n",
    "        text_directory = \"/media/hanakawalab/Transcend/THINGS-Data/THINGS-EEG_images_set/test_images\"\n",
    "\n",
    "    dirnames = [d for d in os.listdir(text_directory) if os.path.isdir(os.path.join(text_directory, d))]\n",
    "    dirnames.sort()\n",
    "\n",
    "    if classes is not None:\n",
    "        dirnames = [dirnames[i] for i in classes]\n",
    "\n",
    "    for dir in dirnames:\n",
    "\n",
    "        try:\n",
    "            idx = dir.index('_')\n",
    "            description = dir[idx+1:]\n",
    "        except ValueError:\n",
    "            print(f\"Skipped: {dir} due to no '_' found.\")\n",
    "            continue\n",
    "\n",
    "        new_description = f\"{description}\"\n",
    "        texts.append(new_description)\n",
    "\n",
    "    if train:\n",
    "        img_directory = \"/media/hanakawalab/Transcend/THINGS-Data/THINGS-EEG_images_set/training_images\"\n",
    "    else:\n",
    "        img_directory =\"/media/hanakawalab/Transcend/THINGS-Data/THINGS-EEG_images_set/test_images\"\n",
    "\n",
    "    all_folders = [d for d in os.listdir(img_directory) if os.path.isdir(os.path.join(img_directory, d))]\n",
    "    all_folders.sort()\n",
    "\n",
    "    if classes is not None and pictures is not None:\n",
    "        images = []\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            pic_idx = pictures[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                if pic_idx < len(all_images):\n",
    "                    images.append(os.path.join(folder_path, all_images[pic_idx]))\n",
    "    elif classes is not None and pictures is None:\n",
    "        images = []\n",
    "        for i in range(len(classes)):\n",
    "            class_idx = classes[i]\n",
    "            if class_idx < len(all_folders):\n",
    "                folder = all_folders[class_idx]\n",
    "                folder_path = os.path.join(img_directory, folder)\n",
    "                all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "                all_images.sort()\n",
    "                images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    elif classes is None:\n",
    "        images = []\n",
    "        for folder in all_folders:\n",
    "            folder_path = os.path.join(img_directory, folder)\n",
    "            all_images = [img for img in os.listdir(folder_path) if img.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "            all_images.sort()\n",
    "            images.extend(os.path.join(folder_path, img) for img in all_images)\n",
    "    else:\n",
    "\n",
    "        print(\"Error\")\n",
    "    return texts, images\n",
    "texts, images = load_data()\n",
    "# images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 3214978\n",
      "self.subjects ['sub-08']\n",
      "exclude_subject None\n",
      "Data tensor shape: torch.Size([200, 63, 250]), label tensor shape: torch.Size([200]), text length: 200, image length: 200\n",
      "torch.Size([200, 63, 250])\n",
      "features_tensor torch.Size([200, 1024])\n",
      " - Test Loss: 1.8868, Test Accuracy: 0.4300\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"] = \"KEY\"\n",
    "os.environ[\"WANDB_MODE\"] = 'offline'\n",
    "from itertools import combinations\n",
    "\n",
    "import clip\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import tqdm\n",
    "from eegdatasets_leaveone import EEGDataset\n",
    "\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "from util import wandb_logger\n",
    "from braindecode.models import EEGNetv4, ATCNet, EEGConformer, EEGITNet, ShallowFBCSPNet\n",
    "import csv\n",
    "from torch import Tensor\n",
    "import itertools\n",
    "import math\n",
    "import re\n",
    "from subject_layers.Transformer_EncDec import Encoder, EncoderLayer\n",
    "from subject_layers.SelfAttention_Family import FullAttention, AttentionLayer\n",
    "from subject_layers.Embed import DataEmbedding\n",
    "import numpy as np\n",
    "from loss import ClipLoss, FeatureContrastLoss\n",
    "import argparse\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from pytorch_wavelets import DWT1DForward, DWT1DInverse\n",
    "\n",
    "class WaveBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(WaveBlock, self).__init__()\n",
    "        self.dwt = DWT1DForward(J=1, wave='db1', mode='zero')\n",
    "        self.dwt.to('cuda')\n",
    "        self.conv = nn.Conv1d(c_in, c_out, 3, 1, padding=1)\n",
    "    def forward(self, x):\n",
    "        xl, xh = self.dwt(x)\n",
    "\n",
    "        x2 = torch.cat((xl, xh[0]), dim=-1)\n",
    "        x3 = self.conv(x2)\n",
    "        return x3\n",
    "\n",
    "class iWaveBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out):\n",
    "        super(iWaveBlock, self).__init__()\n",
    "\n",
    "        self.idwt = DWT1DInverse(wave='db1', mode='zero')\n",
    "        self.idwt.to('cuda')\n",
    "\n",
    "    def forward(self, x ):\n",
    "        xl, xh = torch.split(x, [128, 128], dim=-1)\n",
    "        x4 = self.idwt([xl, [xh]])\n",
    "        return x4\n",
    "\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'classification'  # Example task name\n",
    "        self.seq_len = 250  # Sequence length\n",
    "        self.pred_len = 256  # Prediction length\n",
    "        self.output_attention = False  # Whether to output attention weights\n",
    "        self.d_model = 256  # Model dimension\n",
    "        self.embed = 'timeF'  # Time encoding method\n",
    "        self.freq = 'h'  # Time frequency\n",
    "        self.dropout = 0.25  # Dropout rate\n",
    "        self.factor = 1  # Attention scaling factor\n",
    "        self.n_heads = 4  # Number of attention heads\n",
    "        self.e_layers = 1  # Number of encoder layers\n",
    "        self.d_ff = 256  # Feedforward network dimension\n",
    "        self.activation = 'gelu'  # Activation function\n",
    "        self.enc_in = 63  # Encoder input dimension (example value)\n",
    "\n",
    "\n",
    "class iTransformer(nn.Module):\n",
    "    def __init__(self, configs, joint_train=False, num_subjects=10):\n",
    "        super(iTransformer, self).__init__()\n",
    "        self.task_name = configs.task_name\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.output_attention = configs.output_attention\n",
    "        # Embedding\n",
    "        self.enc_embedding = DataEmbedding(configs.seq_len, configs.d_model, configs.embed, configs.freq,\n",
    "                                           configs.dropout, joint_train=False, num_subjects=num_subjects)\n",
    "\n",
    "        self.dwt = WaveBlock(64, 64)\n",
    "        # Encoder\n",
    "        self.encoder = Encoder(\n",
    "            [\n",
    "                EncoderLayer(\n",
    "                    AttentionLayer(\n",
    "                        FullAttention(False, configs.factor, attention_dropout=configs.dropout,\n",
    "                                      output_attention=configs.output_attention),\n",
    "                        configs.d_model, configs.n_heads\n",
    "                    ),\n",
    "                    configs.d_model,\n",
    "                    configs.d_ff,\n",
    "                    dropout=configs.dropout,\n",
    "                    activation=configs.activation\n",
    "                ) for l in range(configs.e_layers)\n",
    "            ],\n",
    "            norm_layer=torch.nn.LayerNorm(configs.d_model)\n",
    "        )\n",
    "        self.local_conv_time = nn.Conv1d(63, 63, kernel_size=3, padding=3 // 2,\n",
    "                                         groups=63)\n",
    "        # Cross-electrode convolution\n",
    "        self.local_conv_electrode = nn.Conv2d(1, 1, kernel_size=(3, 1), padding=(3 // 2, 0),\n",
    "                                              groups=1)\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Conv1d(63, 63, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.idwt = iWaveBlock(63, 63)\n",
    "        self.conv = nn.Conv1d(63, 63, 4, 2, padding=1)\n",
    "    def forward(self, x_enc, x_mark_enc, subject_ids=None):\n",
    "        # Embedding\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc, subject_ids)\n",
    "        a = self.dwt(enc_out)\n",
    "        a = a[:, :63, :]\n",
    "        local_time = self.local_conv_time(a)\n",
    "        local_time_expanded = local_time.unsqueeze(1)  # (Batch, 1, Channels, Signal_Length)\n",
    "        local_electrode = self.local_conv_electrode(local_time_expanded).squeeze(1)  # (Batch, Channels, Signal_Length)\n",
    "        out, attns = self.encoder(a, attn_mask=None)\n",
    "        out = out[:, :63, :]\n",
    "        weight = self.gate(a)  # (Batch, Channels, 1)\n",
    "        out = local_electrode * (1 - weight) + out * weight\n",
    "        b = self.idwt(out)\n",
    "        c = self.conv(torch.cat([out, b], dim=-1))\n",
    "        #print(\"c\", c.shape)\n",
    "        return c\n",
    "\n",
    "\n",
    "class PrintShape(nn.Module):\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(f\"{self.name} shape: {x.shape}\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class MultiBranchNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_channels=63,\n",
    "                 n_filters1=40,\n",
    "                 n_filters2=40,\n",
    "                 dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # 时间特征分支（基于ShallowNet的设计）\n",
    "        self.temporal_branch = nn.Sequential(\n",
    "            # 时间卷积\n",
    "            nn.Conv2d(in_channels, n_filters1, kernel_size=(1, 25), padding='same'),\n",
    "            nn.BatchNorm2d(n_filters1),\n",
    "            nn.ELU(),\n",
    "            # 平均池化\n",
    "            nn.AvgPool2d((1, 4), stride=(1, 2)),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # 点卷积\n",
    "            nn.Conv2d(n_filters1, n_filters1, kernel_size=(1, 1)),\n",
    "            nn.BatchNorm2d(n_filters1),\n",
    "            nn.ELU(),\n",
    "            # 最终池化\n",
    "            nn.AdaptiveAvgPool2d((1, 37))\n",
    "        )\n",
    "\n",
    "        # 空间特征分支（关注通道间关系）\n",
    "        self.spatial_branch = nn.Sequential(\n",
    "            # 空间卷积（跨通道）\n",
    "            nn.Conv2d(in_channels, n_filters1, kernel_size=(3, 1), padding=(1, 0)),\n",
    "            nn.BatchNorm2d(n_filters1),\n",
    "            nn.ELU(),\n",
    "            # 深度可分离卷积\n",
    "            nn.Conv2d(n_filters1, n_filters1 * 2, kernel_size=(3, 3),\n",
    "                      padding='same', groups=n_filters1),\n",
    "            nn.BatchNorm2d(n_filters1 * 2),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(n_filters1 * 2, n_filters1, kernel_size=1),\n",
    "            nn.BatchNorm2d(n_filters1),\n",
    "            nn.ELU(),\n",
    "            # 最终池化\n",
    "            nn.AdaptiveAvgPool2d((1, 37))\n",
    "        )\n",
    "\n",
    "        # 特征聚合分支\n",
    "        self.fusion_branch = nn.Sequential(\n",
    "            # 合并特征\n",
    "            nn.Conv2d(n_filters1 * 2, n_filters2, kernel_size=1),\n",
    "            nn.BatchNorm2d(n_filters2),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            # 确保输出维度\n",
    "            nn.AdaptiveAvgPool2d((1, 37))\n",
    "        )\n",
    "\n",
    "        # 注意力机制\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(n_filters1 * 2, n_filters1 // 2, kernel_size=1),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(n_filters1 // 2, n_filters1 * 2, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, 40, (1, 1), stride=(1, 1)),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 输入: [batch_size, channels, 1, time]\n",
    "        x = x.unsqueeze(2)\n",
    "        # 1. 时间特征提取\n",
    "        temporal_features = self.temporal_branch(x)\n",
    "\n",
    "        # 2. 空间特征提取\n",
    "        spatial_features = self.spatial_branch(x)\n",
    "\n",
    "        # 3. 特征拼接\n",
    "        combined_features = torch.cat([temporal_features, spatial_features], dim=1)\n",
    "\n",
    "        # 4. 应用注意力机制\n",
    "        attention_weights = self.attention(combined_features)\n",
    "        weighted_features = combined_features * attention_weights\n",
    "\n",
    "        # 5. 特征融合\n",
    "        output = self.fusion_branch(weighted_features)\n",
    "        output = self.projection(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "# 辅助模块：自定义的深度可分离卷积块\n",
    "class DepthwiseSeparableConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size):\n",
    "        super().__init__()\n",
    "        self.depthwise = nn.Conv2d(\n",
    "            in_channels, in_channels, kernel_size=kernel_size,\n",
    "            padding='same', groups=in_channels\n",
    "        )\n",
    "        self.pointwise = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.depthwise(x)\n",
    "        x = self.pointwise(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, emb_size=40):\n",
    "        super().__init__()\n",
    "        # Revised from ShallowNet\n",
    "        self.tsconv = nn.Sequential(\n",
    "            nn.Conv2d(1, 40, (1, 25), stride=(1, 1)),\n",
    "            nn.AvgPool2d((1, 51), (1, 5)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Conv2d(40, 40, (63, 1), stride=(1, 1)),\n",
    "            nn.BatchNorm2d(40),\n",
    "            nn.ELU(),\n",
    "            nn.Dropout(0.5),\n",
    "        )\n",
    "\n",
    "        self.projection = nn.Sequential(\n",
    "            nn.Conv2d(40, emb_size, (1, 1), stride=(1, 1)),\n",
    "            Rearrange('b e (h) (w) -> b (h w) e'),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        # b, _, _, _ = x.shape\n",
    "        x = x.unsqueeze(1)\n",
    "        #print(\"x\", x.shape)\n",
    "        x = self.tsconv(x)\n",
    "        #print(\"tsconv\", x.shape)\n",
    "        x = self.projection(x)\n",
    "        #print(\"projection\", x.shape)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualAdd(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        res = x\n",
    "        x = self.fn(x, **kwargs)\n",
    "        x += res\n",
    "        return x\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Sequential):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.contiguous().view(x.size(0), -1)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Enc_eeg(nn.Sequential):\n",
    "    def __init__(self, emb_size=40, **kwargs):\n",
    "        super().__init__(\n",
    "            MultiBranchNet(),\n",
    "            FlattenHead()\n",
    "        )\n",
    "\n",
    "\n",
    "class Proj_eeg(nn.Sequential):\n",
    "    def __init__(self, embedding_dim=1480, proj_dim=1024, drop_proj=0.5):\n",
    "        super().__init__(\n",
    "            nn.Linear(embedding_dim, proj_dim),\n",
    "            ResidualAdd(nn.Sequential(\n",
    "                nn.GELU(),\n",
    "                nn.Linear(proj_dim, proj_dim),\n",
    "                nn.Dropout(drop_proj),\n",
    "            )),\n",
    "            nn.LayerNorm(proj_dim),\n",
    "        )\n",
    "\n",
    "\n",
    "class ATMS(nn.Module):\n",
    "    def __init__(self, num_channels=63, sequence_length=250, num_subjects=1, num_features=64, num_latents=2048,\n",
    "                 num_blocks=1):\n",
    "        super(ATMS, self).__init__()\n",
    "        default_config = Config()\n",
    "        self.encoder = iTransformer(default_config)\n",
    "        self.subject_wise_linear = nn.ModuleList(\n",
    "            [nn.Linear(default_config.d_model, sequence_length) for _ in range(num_subjects)])\n",
    "        self.enc_eeg = Enc_eeg()\n",
    "        self.proj_eeg = Proj_eeg()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.loss_func = ClipLoss()\n",
    "        self.loss2 = FeatureContrastLoss()\n",
    "\n",
    "    def forward(self, x, subject_ids):\n",
    "        x = self.encoder(x, None, subject_ids)\n",
    "        # print(f'After attention shape: {x.shape}')\n",
    "        # print(\"x\", x.shape)\n",
    "        # x = self.subject_wise_linear[0](x)\n",
    "        # print(f'After subject-specific linear transformation shape: {x.shape}')\n",
    "        eeg_embedding = self.enc_eeg(x)\n",
    "\n",
    "        out = self.proj_eeg(eeg_embedding)\n",
    "        return out\n",
    "\n",
    "def extract_id_from_string(s):\n",
    "    match = re.search(r'\\d+$', s)\n",
    "    if match:\n",
    "        return int(match.group())\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_eegfeatures_test(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    beta = 0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    loss_CF = FeatureContrastLoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            print(eeg_data.shape)\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "\n",
    "            batch_size = eeg_data.size(0)  # Assume the first element is the data tensor\n",
    "            subject_id = extract_id_from_string(sub)\n",
    "            # eeg_data = eeg_data.permute(0, 2, 1)\n",
    "            subject_ids = torch.full((batch_size,), subject_id, dtype=torch.long).to(device)\n",
    "            # if not config.insubject:\n",
    "            #     subject_ids = torch.full((batch_size,), -1, dtype=torch.long).to(device)\n",
    "            eeg_features = eeg_model(eeg_data, subject_ids)\n",
    "            features_list.append(eeg_features.cpu())\n",
    "\n",
    "            logit_scale = eeg_model.logit_scale\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)\n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)\n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            loss = beta * loss + (1 - beta) * loss_CF(eeg_features, labels)\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "\n",
    "\n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"20250103_ATM_S_eeg_features_{sub}-test.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "from IPython.display import Image, display\n",
    "config = {\n",
    "\"data_path\": \"/media/hanakawalab/Transcend/THINGS-Data/EEG/Preprocessed_data_250Hz/\",\n",
    "\"project\": \"train_pos_img_text_rep\",\n",
    "\"entity\": \"sustech_rethinkingbci\",\n",
    "\"name\": \"lr=3e-4_img_pos_pro_eeg\",\n",
    "\"lr\": 3e-4,\n",
    "\"epochs\": 50,\n",
    "\"batch_size\": 1024,\n",
    "\"logger\": True,\n",
    "\"encoder_type\":'ATMS',\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "data_path = config['data_path']\n",
    "emb_img_test = torch.load('variables/ViT-H-14_features_test.pt')\n",
    "emb_img_train = torch.load('variables/ViT-H-14_features_train.pt')\n",
    "\n",
    "eeg_model = ATMS(63, 250)\n",
    "print('number of parameters:', sum([p.numel() for p in eeg_model.parameters()]))\n",
    "\n",
    "#####################################################################################\n",
    "\n",
    "# eeg_model.load_state_dict(torch.load(\"/home/ldy/Workspace/Reconstruction/models/contrast/sub-08/01-30_00-44/40.pth\"))\n",
    "eeg_model.load_state_dict(torch.load(\"models/contrast/ATMS/sub-08/01-03_10-18/15.pth\"))\n",
    "eeg_model = eeg_model.to(device)\n",
    "sub = 'sub-08'\n",
    "#####################################################################################\n",
    "\n",
    "test_dataset = EEGDataset(data_path, subjects= [sub], train=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = test_dataset.text_features\n",
    "img_features_test_all = test_dataset.img_features\n",
    "test_loss, test_accuracy,labels, eeg_features_test = get_eegfeatures_test(sub, eeg_model, test_loader, device, text_features_test_all, img_features_test_all,k=200)\n",
    "print(f\" - Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:27:43.154653Z",
     "end_time": "2025-01-03T10:27:45.207896Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.subjects ['sub-08']\n",
      "exclude_subject None\n",
      "data_tensor torch.Size([66160, 63, 250])\n",
      "Data tensor shape: torch.Size([66160, 63, 250]), label tensor shape: torch.Size([66160]), text length: 1654, image length: 16540\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([1024, 63, 250])\n",
      "torch.Size([624, 63, 250])\n",
      "features_tensor torch.Size([66160, 1024])\n",
      " - Test Loss: 3.0236, Test Accuracy: 0.0046\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def get_eegfeatures_train(sub, eegmodel, dataloader, device, text_features_all, img_features_all, k):\n",
    "    eegmodel.eval()\n",
    "    text_features_all = text_features_all.to(device).float()\n",
    "    img_features_all = img_features_all.to(device).float()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    alpha =0.9\n",
    "    beta = 0.9\n",
    "    top5_correct = 0\n",
    "    top5_correct_count = 0\n",
    "\n",
    "    all_labels = set(range(text_features_all.size(0)))\n",
    "    top5_acc = 0\n",
    "    mse_loss_fn = nn.MSELoss()\n",
    "    loss_CF = FeatureContrastLoss()\n",
    "    ridge_lambda = 0.1\n",
    "    save_features = True\n",
    "    features_list = []  # List to store features\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (eeg_data, labels, text, text_features, img, img_features) in enumerate(dataloader):\n",
    "            print(eeg_data.shape)\n",
    "            eeg_data = eeg_data.to(device)\n",
    "            text_features = text_features.to(device).float()\n",
    "            labels = labels.to(device)\n",
    "            img_features = img_features.to(device).float()\n",
    "\n",
    "            batch_size = eeg_data.size(0)  # Assume the first element is the data tensor\n",
    "            subject_id = extract_id_from_string(sub)\n",
    "            # eeg_data = eeg_data.permute(0, 2, 1)\n",
    "            subject_ids = torch.full((batch_size,), subject_id, dtype=torch.long).to(device)\n",
    "            # if not config.insubject:\n",
    "            #     subject_ids = torch.full((batch_size,), -1, dtype=torch.long).to(device)\n",
    "            eeg_features = eeg_model(eeg_data, subject_ids)\n",
    "            features_list.append(eeg_features.cpu())\n",
    "\n",
    "            logit_scale = eeg_model.logit_scale\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"eeg_features\", eeg_features.shape)\n",
    "            # print(torch.std(eeg_features, dim=-1))\n",
    "            # print(torch.std(img_features, dim=-1))\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)\n",
    "            img_loss = eegmodel.loss_func(eeg_features, img_features, logit_scale)\n",
    "            text_loss = eegmodel.loss_func(eeg_features, text_features, logit_scale)\n",
    "            contrastive_loss = img_loss\n",
    "            # loss = img_loss + text_loss\n",
    "\n",
    "            regress_loss =  mse_loss_fn(eeg_features, img_features)\n",
    "            # print(\"text_loss\", text_loss)\n",
    "            # print(\"img_loss\", img_loss)\n",
    "            # print(\"regress_loss\", regress_loss)\n",
    "            # l2_norm = sum(p.pow(2.0).sum() for p in model.parameters())\n",
    "            # loss = (regress_loss + ridge_lambda * l2_norm)\n",
    "            loss = alpha * regress_loss *10 + (1 - alpha) * contrastive_loss*10\n",
    "            loss = beta * loss + (1 - beta) * loss_CF(eeg_features, labels)\n",
    "            # print(\"loss\", loss)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            for idx, label in enumerate(labels):\n",
    "\n",
    "                possible_classes = list(all_labels - {label.item()})\n",
    "                selected_classes = random.sample(possible_classes, k-1) + [label.item()]\n",
    "                selected_img_features = img_features_all[selected_classes]\n",
    "\n",
    "\n",
    "                logits_img = logit_scale * eeg_features[idx] @ selected_img_features.T\n",
    "                # logits_text = logit_scale * eeg_features[idx] @ selected_text_features.T\n",
    "                # logits_single = (logits_text + logits_img) / 2.0\n",
    "                logits_single = logits_img\n",
    "                # print(\"logits_single\", logits_single.shape)\n",
    "\n",
    "                # predicted_label = selected_classes[torch.argmax(logits_single).item()]\n",
    "                predicted_label = selected_classes[torch.argmax(logits_single).item()] # (n_batch, ) \\in {0, 1, ..., n_cls-1}\n",
    "                if predicted_label == label.item():\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "        if save_features:\n",
    "            features_tensor = torch.cat(features_list, dim=0)\n",
    "            print(\"features_tensor\", features_tensor.shape)\n",
    "            torch.save(features_tensor.cpu(), f\"20250103_ATM_S_eeg_features_{sub}.pt\")  # Save features as .pt file\n",
    "    average_loss = total_loss / (batch_idx+1)\n",
    "    accuracy = correct / total\n",
    "    return average_loss, accuracy, labels, features_tensor.cpu()\n",
    "\n",
    "#####################################################################################\n",
    "train_dataset = EEGDataset(data_path, subjects= [sub], train=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=config[\"batch_size\"], shuffle=False, num_workers=0)\n",
    "text_features_test_all = train_dataset.text_features\n",
    "img_features_test_all = train_dataset.img_features\n",
    "\n",
    "train_loss, train_accuracy, labels, eeg_features_train = get_eegfeatures_train(sub, eeg_model, train_loader, device, text_features_test_all, img_features_test_all,k=200)\n",
    "print(f\" - Test Loss: {train_loss:.4f}, Test Accuracy: {train_accuracy:.4f}\")\n",
    "#####################################################################################"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:27:48.520932Z",
     "end_time": "2025-01-03T10:28:04.857237Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import open_clip\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "import sys\n",
    "from diffusion_prior import *\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:30:11.670000Z",
     "end_time": "2025-01-03T10:30:11.711691Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "emb_img_train_4 = emb_img_train.view(1654,10,1,1024).repeat(1,1,4,1).view(-1,1024)\n",
    "emb_eeg = torch.load('/media/hanakawalab/Transcend/EEG_Image_decode-main/Generation/20250103_ATM_S_eeg_features_sub-08.pt')\n",
    "emb_eeg_test = torch.load('/media/hanakawalab/Transcend/EEG_Image_decode-main/Generation/20250103_ATM_S_eeg_features_sub-08-test.pt')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:30:12.108749Z",
     "end_time": "2025-01-03T10:30:12.386110Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9675648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanakawalab/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 64 worker processes in total. Our suggested max number of worker in current system is 20, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": "<All keys matched successfully>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = EmbeddingDataset(\n",
    "    c_embeddings=eeg_features_train, h_embeddings=emb_img_train_4,\n",
    "    # h_embeds_uncond=h_embeds_imgnet\n",
    ")\n",
    "dl = DataLoader(dataset, batch_size=1024, shuffle=True, num_workers=64)\n",
    "diffusion_prior = DiffusionPriorUNet(cond_dim=1024, dropout=0.1)\n",
    "# number of parameters\n",
    "print(sum(p.numel() for p in diffusion_prior.parameters() if p.requires_grad))\n",
    "pipe = Pipe(diffusion_prior, device=device)\n",
    "\n",
    "# load pretrained model\n",
    "model_name = 'diffusion_prior' # 'diffusion_prior_vice_pre_imagenet' or 'diffusion_prior_vice_pre'\n",
    "#pipe.train(dl, num_epochs=150, learning_rate=1e-3) # to 0.142\n",
    "string = f'/media/hanakawalab/Transcend/EEG_Image_decode/fintune_ckpts/{sub}/{model_name}.pt'\n",
    "pipe.diffusion_prior.load_state_dict(torch.load(string, map_location=device))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-01-03T10:30:12.938006Z",
     "end_time": "2025-01-03T10:30:13.133519Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanakawalab/miniconda3/envs/BCI/lib/python3.10/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d409292deee140dabca2246624e7a5dc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [00:00, 637.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62160b6a70d7469588288efa458778fa"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.69 GiB total capacity; 14.52 GiB already allocated; 25.50 MiB free; 14.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 15\u001B[0m\n\u001B[1;32m     13\u001B[0m h \u001B[38;5;241m=\u001B[39m pipe\u001B[38;5;241m.\u001B[39mgenerate(c_embeds\u001B[38;5;241m=\u001B[39meeg_embeds, num_inference_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, guidance_scale\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5.0\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m---> 15\u001B[0m     image \u001B[38;5;241m=\u001B[39m \u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mh\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat16\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;66;03m# Construct the save path for each image\u001B[39;00m\n\u001B[1;32m     17\u001B[0m     path \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdirectory\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtexts[k]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mj\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;124m'\u001B[39m\n",
      "File \u001B[0;32m/media/hanakawalab/Transcend/EEG_Image_decode-main/Generation/custom_pipeline.py:492\u001B[0m, in \u001B[0;36mGenerator4Embeds.generate\u001B[0;34m(self, image_embeds, text_prompt, generator)\u001B[0m\n\u001B[1;32m    489\u001B[0m pipe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpipe\n\u001B[1;32m    491\u001B[0m \u001B[38;5;66;03m# generate image with image prompt - ip_adapter_embeds\u001B[39;00m\n\u001B[0;32m--> 492\u001B[0m image \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_ip_adapter_embeds\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    493\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprompt\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtext_prompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    494\u001B[0m \u001B[43m    \u001B[49m\u001B[43mip_adapter_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mimage_embeds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[1;32m    495\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_inference_steps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnum_inference_steps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    496\u001B[0m \u001B[43m    \u001B[49m\u001B[43mguidance_scale\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m0.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mgenerator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mgenerator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    498\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mimages[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    500\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m image\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001B[0m, in \u001B[0;36mcontext_decorator.<locals>.decorate_context\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    112\u001B[0m \u001B[38;5;129m@functools\u001B[39m\u001B[38;5;241m.\u001B[39mwraps(func)\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdecorate_context\u001B[39m(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m    114\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m ctx_factory():\n\u001B[0;32m--> 115\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/media/hanakawalab/Transcend/EEG_Image_decode-main/Generation/custom_pipeline.py:365\u001B[0m, in \u001B[0;36mgenerate_ip_adapter_embeds\u001B[0;34m(self, prompt, prompt_2, height, width, num_inference_steps, timesteps, denoising_end, guidance_scale, negative_prompt, negative_prompt_2, num_images_per_prompt, eta, generator, latents, prompt_embeds, negative_prompt_embeds, pooled_prompt_embeds, negative_pooled_prompt_embeds, ip_adapter_image, ip_adapter_embeds, output_type, return_dict, cross_attention_kwargs, guidance_rescale, original_size, crops_coords_top_left, target_size, negative_original_size, negative_crops_coords_top_left, negative_target_size, clip_skip, callback_on_step_end, callback_on_step_end_tensor_inputs, **kwargs)\u001B[0m\n\u001B[1;32m    363\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ip_adapter_image \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m ip_adapter_embeds \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    364\u001B[0m     added_cond_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage_embeds\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m image_embeds\n\u001B[0;32m--> 365\u001B[0m noise_pred \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munet\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    366\u001B[0m \u001B[43m    \u001B[49m\u001B[43mlatent_model_input\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    367\u001B[0m \u001B[43m    \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    368\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprompt_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    369\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtimestep_cond\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimestep_cond\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    370\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    371\u001B[0m \u001B[43m    \u001B[49m\u001B[43madded_cond_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madded_cond_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    372\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    373\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    375\u001B[0m \u001B[38;5;66;03m# perform guidance\u001B[39;00m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdo_classifier_free_guidance:\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/unet_2d_condition.py:1075\u001B[0m, in \u001B[0;36mUNet2DConditionModel.forward\u001B[0;34m(self, sample, timestep, encoder_hidden_states, class_labels, timestep_cond, attention_mask, cross_attention_kwargs, added_cond_kwargs, down_block_additional_residuals, mid_block_additional_residual, down_intrablock_additional_residuals, encoder_attention_mask, return_dict)\u001B[0m\n\u001B[1;32m   1072\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m is_adapter \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(down_intrablock_additional_residuals) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m   1073\u001B[0m         additional_residuals[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madditional_residuals\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m down_intrablock_additional_residuals\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m-> 1075\u001B[0m     sample, res_samples \u001B[38;5;241m=\u001B[39m \u001B[43mdownsample_block\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1076\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msample\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1077\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemb\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43memb\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1078\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1079\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1080\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1081\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1082\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43madditional_residuals\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1083\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1084\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1085\u001B[0m     sample, res_samples \u001B[38;5;241m=\u001B[39m downsample_block(hidden_states\u001B[38;5;241m=\u001B[39msample, temb\u001B[38;5;241m=\u001B[39memb, scale\u001B[38;5;241m=\u001B[39mlora_scale)\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/unet_2d_blocks.py:1160\u001B[0m, in \u001B[0;36mCrossAttnDownBlock2D.forward\u001B[0;34m(self, hidden_states, temb, encoder_hidden_states, attention_mask, cross_attention_kwargs, encoder_attention_mask, additional_residuals)\u001B[0m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1159\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m resnet(hidden_states, temb, scale\u001B[38;5;241m=\u001B[39mlora_scale)\n\u001B[0;32m-> 1160\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[43m        \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1166\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1167\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;66;03m# apply additional residuals to the output of the last pair of resnet and attention blocks\u001B[39;00m\n\u001B[1;32m   1170\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mlen\u001B[39m(blocks) \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m additional_residuals \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/transformer_2d.py:392\u001B[0m, in \u001B[0;36mTransformer2DModel.forward\u001B[0;34m(self, hidden_states, encoder_hidden_states, timestep, added_cond_kwargs, class_labels, cross_attention_kwargs, attention_mask, encoder_attention_mask, return_dict)\u001B[0m\n\u001B[1;32m    380\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39mcheckpoint\u001B[38;5;241m.\u001B[39mcheckpoint(\n\u001B[1;32m    381\u001B[0m             create_custom_forward(block),\n\u001B[1;32m    382\u001B[0m             hidden_states,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    389\u001B[0m             \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mckpt_kwargs,\n\u001B[1;32m    390\u001B[0m         )\n\u001B[1;32m    391\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 392\u001B[0m         hidden_states \u001B[38;5;241m=\u001B[39m \u001B[43mblock\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    393\u001B[0m \u001B[43m            \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    394\u001B[0m \u001B[43m            \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    395\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    396\u001B[0m \u001B[43m            \u001B[49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_attention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    397\u001B[0m \u001B[43m            \u001B[49m\u001B[43mtimestep\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimestep\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    398\u001B[0m \u001B[43m            \u001B[49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    399\u001B[0m \u001B[43m            \u001B[49m\u001B[43mclass_labels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mclass_labels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    400\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[38;5;66;03m# 3. Output\u001B[39;00m\n\u001B[1;32m    403\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_input_continuous:\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/attention.py:288\u001B[0m, in \u001B[0;36mBasicTransformerBlock.forward\u001B[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, timestep, cross_attention_kwargs, class_labels)\u001B[0m\n\u001B[1;32m    285\u001B[0m cross_attention_kwargs \u001B[38;5;241m=\u001B[39m cross_attention_kwargs\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m cross_attention_kwargs \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m {}\n\u001B[1;32m    286\u001B[0m gligen_kwargs \u001B[38;5;241m=\u001B[39m cross_attention_kwargs\u001B[38;5;241m.\u001B[39mpop(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgligen\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m)\n\u001B[0;32m--> 288\u001B[0m attn_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattn1\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnorm_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43monly_cross_attention\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    291\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    292\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    293\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    294\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39muse_ada_layer_norm_zero:\n\u001B[1;32m    295\u001B[0m     attn_output \u001B[38;5;241m=\u001B[39m gate_msa\u001B[38;5;241m.\u001B[39munsqueeze(\u001B[38;5;241m1\u001B[39m) \u001B[38;5;241m*\u001B[39m attn_output\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/attention_processor.py:522\u001B[0m, in \u001B[0;36mAttention.forward\u001B[0;34m(self, hidden_states, encoder_hidden_states, attention_mask, **cross_attention_kwargs)\u001B[0m\n\u001B[1;32m    503\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    504\u001B[0m \u001B[38;5;124;03mThe forward method of the `Attention` class.\u001B[39;00m\n\u001B[1;32m    505\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    517\u001B[0m \u001B[38;5;124;03m    `torch.Tensor`: The output of the attention layer.\u001B[39;00m\n\u001B[1;32m    518\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    519\u001B[0m \u001B[38;5;66;03m# The `Attention` class can call different attention processors / attention functions\u001B[39;00m\n\u001B[1;32m    520\u001B[0m \u001B[38;5;66;03m# here we simply pass along all tensors to the selected processor class\u001B[39;00m\n\u001B[1;32m    521\u001B[0m \u001B[38;5;66;03m# For standard processors that are defined here, `**cross_attention_kwargs` is empty\u001B[39;00m\n\u001B[0;32m--> 522\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mprocessor\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    524\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    525\u001B[0m \u001B[43m    \u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    526\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    527\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcross_attention_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    528\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/attention_processor.py:1218\u001B[0m, in \u001B[0;36mAttnProcessor2_0.__call__\u001B[0;34m(self, attn, hidden_states, encoder_hidden_states, attention_mask, temb, scale)\u001B[0m\n\u001B[1;32m   1215\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m attn\u001B[38;5;241m.\u001B[39mnorm_cross:\n\u001B[1;32m   1216\u001B[0m     encoder_hidden_states \u001B[38;5;241m=\u001B[39m attn\u001B[38;5;241m.\u001B[39mnorm_encoder_hidden_states(encoder_hidden_states)\n\u001B[0;32m-> 1218\u001B[0m key \u001B[38;5;241m=\u001B[39m \u001B[43mattn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_k\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder_hidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1219\u001B[0m value \u001B[38;5;241m=\u001B[39m attn\u001B[38;5;241m.\u001B[39mto_v(encoder_hidden_states, \u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m   1221\u001B[0m inner_dim \u001B[38;5;241m=\u001B[39m key\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1496\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1497\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1498\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1499\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1500\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1501\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1502\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1503\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/diffusers/models/lora.py:430\u001B[0m, in \u001B[0;36mLoRACompatibleLinear.forward\u001B[0;34m(self, hidden_states, scale)\u001B[0m\n\u001B[1;32m    428\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor, scale: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1.0\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m    429\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlora_layer \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 430\u001B[0m         out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    431\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m out\n\u001B[1;32m    432\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/miniconda3/envs/BCI/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 15.69 GiB total capacity; 14.52 GiB already allocated; 25.50 MiB free; 14.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from custom_pipeline import *\n",
    "# Create the directory if it doesn't exist\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# Assuming generator.generate returns a PIL Image\n",
    "generator = Generator4Embeds(num_inference_steps=4, device=device)\n",
    "\n",
    "directory = f\"generated_imgs/{sub}\"\n",
    "os.makedirs(directory, exist_ok=True)\n",
    "for k in range(200):\n",
    "    eeg_embeds = emb_eeg_test[k:k+1]\n",
    "    h = pipe.generate(c_embeds=eeg_embeds, num_inference_steps=50, guidance_scale=5.0)\n",
    "    for j in range(10):\n",
    "        image = generator.generate(h.to(dtype=torch.float16))\n",
    "        # Construct the save path for each image\n",
    "        path = f'{directory}/{texts[k]}/{j}.png'\n",
    "        # Ensure the directory exists\n",
    "        os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "        # Save the PIL Image\n",
    "        image.save(path)\n",
    "        print(f'Image saved to {path}')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
